{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10378fc5-0142-484f-a8f0-e714026870c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from math import exp\n",
    "from collections import defaultdict, Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49b210b2-c1e2-4060-977e-3f2f1cb910da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mixed_questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "questions = data[\"questions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21bd31d6-415e-4012-9b7a-31391977c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_year = 2027     # year to predict for\n",
    "lambda_decay = 3       # recency decay factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3867be0a-3b0c-4dde-a980-e5bb8352818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO compute last_seen_year per concept\n",
    "\n",
    "# concept_last_seen_year[cid] = latest year an exam question appeared in this concept\n",
    "concept_last_seen_year = {}\n",
    "\n",
    "for q in questions:\n",
    "    if q.get(\"source\", \"\").lower() == \"exam\":\n",
    "        cid = q[\"concept_id\"]\n",
    "        # Use existing last_seen_year if present, else fallback to 'year'\n",
    "        year = q.get(\"derived_features\", {}).get(\"last_seen_year\", q.get(\"year\"))\n",
    "        if year is not None:\n",
    "            concept_last_seen_year[cid] = max(concept_last_seen_year.get(cid, year), year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb4923f-646d-4817-bb1d-a803efb1e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in questions:\n",
    "    cid = q[\"concept_id\"]\n",
    "\n",
    "    # Ensure derived_features dictionary exists\n",
    "    if \"derived_features\" not in q:\n",
    "        q[\"derived_features\"] = {}\n",
    "\n",
    "    # Concept-level last_seen_year\n",
    "    concept_year = concept_last_seen_year.get(cid, None)\n",
    "\n",
    "    if concept_year is not None:\n",
    "        gap = target_year - concept_year\n",
    "        q[\"derived_features\"][\"gap_since_last_seen\"] = gap\n",
    "        q[\"derived_features\"][\"recency_decay\"] = exp(-gap / lambda_decay)\n",
    "        q[\"derived_features\"][\"concept_last_seen_year\"] = concept_year\n",
    "\n",
    "        # Exam questions keep their original last_seen_year\n",
    "        if q.get(\"source\", \"\").lower() == \"exam\":\n",
    "            q[\"derived_features\"][\"last_seen_year\"] = q[\"derived_features\"].get(\"last_seen_year\", q.get(\"year\"))\n",
    "\n",
    "    else:\n",
    "        # Concept never appeared in exams\n",
    "        q[\"derived_features\"][\"gap_since_last_seen\"] = None\n",
    "        q[\"derived_features\"][\"recency_decay\"] = 0.0\n",
    "        q[\"derived_features\"][\"concept_last_seen_year\"] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86e53b6c-cde5-457e-ad1a-37bd94f6ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#section frequency table (from exam questions only)\n",
    "\n",
    "cluster_section_counts = defaultdict(lambda: {\"A\": 0, \"B\": 0, \"C\": 0})\n",
    "\n",
    "for q in questions:\n",
    "    if q[\"source\"] == \"exam\":\n",
    "        cid = q[\"concept_id\"]\n",
    "        sec = q[\"exam_meta\"][\"section\"]\n",
    "        cluster_section_counts[cid][sec] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f210d9dc-b99b-4103-bd1f-b878037be918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert counts â†’ probabilities (with smoothing to avoid 0s)\n",
    "\n",
    "cluster_section_probs = {}\n",
    "\n",
    "alpha = 1.0  # smoothing strength\n",
    "\n",
    "for cid, counts in cluster_section_counts.items():\n",
    "    total = sum(counts.values()) + 3 * alpha\n",
    "    cluster_section_probs[cid] = {\n",
    "        \"A\": (counts[\"A\"] + alpha) / total,\n",
    "        \"B\": (counts[\"B\"] + alpha) / total,\n",
    "        \"C\": (counts[\"C\"] + alpha) / total,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15b14bd9-8fb0-4744-93bf-3aa52cca008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global section distribution\n",
    "\n",
    "global_counts = {\"A\": 0, \"B\": 0, \"C\": 0}\n",
    "\n",
    "for q in questions:\n",
    "    if q[\"source\"] == \"exam\":\n",
    "        sec = q[\"exam_meta\"][\"section\"]\n",
    "        global_counts[sec] += 1\n",
    "\n",
    "total = sum(global_counts.values())\n",
    "\n",
    "if total > 0:\n",
    "    global_probs = {\n",
    "        \"A\": global_counts[\"A\"] / total,\n",
    "        \"B\": global_counts[\"B\"] / total,\n",
    "        \"C\": global_counts[\"C\"] / total,\n",
    "    }\n",
    "else:\n",
    "    global_probs = {\"A\": 1/3, \"B\": 1/3, \"C\": 1/3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "473e0176-dcb0-4958-b70f-6b14341f089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign section_prob to all questions\n",
    "\n",
    "for q in questions:\n",
    "    cid = q[\"concept_id\"]\n",
    "\n",
    "    # Fallback for noise or unseen clusters\n",
    "    if cid == -1 or cid not in cluster_section_probs:\n",
    "        probs = global_probs\n",
    "    else:\n",
    "        probs = cluster_section_probs[cid]\n",
    "\n",
    "    # Ensure derived_features exists\n",
    "    if \"derived_features\" not in q:\n",
    "        q[\"derived_features\"] = {}\n",
    "\n",
    "    # Store section probabilities\n",
    "    q[\"derived_features\"][\"section_prob\"] = {\n",
    "        \"A\": probs[\"A\"],\n",
    "        \"B\": probs[\"B\"],\n",
    "        \"C\": probs[\"C\"],\n",
    "    }\n",
    "\n",
    "    # Add confidence feature (very useful for ML)\n",
    "    q[\"derived_features\"][\"section_confidence\"] = max(\n",
    "        probs[\"A\"], probs[\"B\"], probs[\"C\"]\n",
    "    )\n",
    "    \n",
    "    # assign most likely section\n",
    "    predicted_section = max(probs, key=probs.get)\n",
    "    # if it's a textbook question, also mirror into exam_meta\n",
    "    if q.get(\"source\", \"\").lower() == \"textbook\":\n",
    "        if q.get(\"exam_meta\") is None:\n",
    "            q[\"exam_meta\"] = {}\n",
    "        q[\"exam_meta\"][\"section\"] = predicted_section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0c8dd9b-94c5-40a7-a6c3-a1c221dce5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"checking.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf98399-08b8-4215-9bf6-4c1dccc28f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
